\section{Introduction}

Neural networks (NNs) increasingly underpin decision-making in safety-critical settings, yet their complexity and opacity complicate the provision of rigorous behavioural guarantees \cite{huang2020survey,liu2021algorithms,JMLR:v25:23-0119}. Empirical testing alone can miss rare but catastrophic corner cases; safety-critical deployment therefore requires methods that can establish \emph{formal} guarantees of correctness and robustness under clearly specified operating conditions.

Formal verification for NNs aims to prove that a property holds for all inputs within a region of interest (e.g., an $\ell_p$-ball around observed data) \cite{katz2019marabou}. The setting differs from traditional software verification both in object (learned, high-dimensional functions) and in failure modes: small, even imperceptible perturbations can induce harmful output deviations \cite{carlini2019evaluating}. A spectrum of techniques has emerged. \emph{Complete} methods—e.g., SMT/MILP encodings in the Reluplex/Marabou lineage—can prove or refute properties but often struggle to scale on modern networks \cite{katz2017reluplex,katz2019marabou,ehlers2017planet,tjeng2019milp}. \emph{Incomplete} methods trade completeness for scalability via abstractions/relaxations, including abstract interpretation and linear/convex relaxations with branch-and-bound (as in $\alpha,\beta$-CROWN, and convex outer polytopes / dual relaxations) \cite{gehr2018ai2,singh2019abstract,zhang2018efficient,xu2021fast,wang2021beta,zhang2019crownibp,wong2018provable,raghunathan2018certified,zhou2024scalable}. Recent DPLL(T)-based approaches (e.g., NeuralSAT) bring clause learning and restarts tailored to NN structure \cite{duong2024dpllt}. Community efforts such as VNN-COMP have accelerated progress through standardised tasks and transparent comparisons \cite{bak2021second,brix2023fourth,brix2024fifth}, while frameworks like DNNV support unified experimentation across diverse tools \cite{shriver2021dnnv}.

Autonomous driving systems (ADS) are a particularly demanding application domain. NN components are pervasive across perception, planning and control \cite{bojarski2016end}, while robust operation must be ensured over high-dimensional sensory inputs and dynamic environments, under real-time constraints \cite{tran2020verification,dreossi2019verifai,koopman2016challenges}. Although verification advances are well documented for image classifiers and selected control modules, there remains an under-documented gap: \emph{end-to-end, development-centred} workflows that integrate simulator-driven data collection, controller training, specification design, and formal verification in a single iterative loop.

This paper presents an end-to-end pipeline for developing, training, and formally verifying vision-based neural controllers in a simulated ADS. We build a Unity-based closed-loop environment and curate a task-specific dataset for a discrete steering controller. We study a family of compact convolutional controllers and propose a set of task-adapted, \emph{scale-invariant} robustness properties tailored to control: label consistency (classification robustness), a relative-confidence constraint (adapted SCR), their conjunction (combined robustness), and per-logit stability. We verify these properties under $\ell_\infty$ perturbations using three complementary verifiers: $\alpha,\beta$-CROWN (linear relaxations + branch-and-bound) \cite{zhang2018efficient,xu2021fast,wang2021beta,zhou2024scalable}, Marabou (SMT-based, complete) \cite{katz2019marabou}, and NeuralSAT (DPLL(T) with clause learning and restarts) \cite{duong2024dpllt}. We further integrate adversarial training into the loop and examine how it affects \emph{verified} robustness (not just empirical accuracy).

\noindent \emph{Empirically}, across four models $\times$ four properties $\times$ four perturbation budgets $\times$ three training radii (Table~\ref{tab:all_models_props}), we observe two robust patterns that motivate the rest of the paper: (i) adversarial training at $25/255$ significantly increases verified-safe rates (on average by $\approx\!40$ points for the label-consistency property and by $\approx\!27$ points for the combined specifications), and (ii) Marabou yields near-zero \emph{Unknown} outcomes overall ($\approx\!0.6\%$) while $\alpha,\beta$-CROWN and NeuralSAT exhibit higher \emph{Unknown} rates at intermediate perturbation budgets ($\approx\!13.5\%$ and $\approx\!10.7\%$, respectively); full details appear in \S\ref{sec:results}.

\paragraph{Contributions.}
\begin{itemize}
  \item \textbf{A verification-centred ADS pipeline.} We integrate simulator-driven data collection, controller training, specification design, and formal verification into a single, iterative workflow suited to controller development and analysis.
  \item \textbf{Task-adapted, scale-invariant specifications.} We refine canonical definitions into ADS-appropriate robustness properties (relative confidence and per-logit stability, plus their conjunction), mitigating brittleness of absolute logit thresholds and aligning verification with control risk \cite{casadio2022neural}.
  \item \textbf{Tri-verifier evaluation within the pipeline.} We exercise three complementary tools—$\alpha,\beta$-CROWN \cite{zhang2018efficient,xu2021fast,wang2021beta,zhou2024scalable}, Marabou \cite{katz2019marabou}, and NeuralSAT \cite{duong2024dpllt}—across model sizes, perturbation budgets and training regimes, highlighting coverage-runtime-unknown trade-offs and typical vs.\ worst-case behaviour \cite{bak2021second,brix2023fourth,brix2024fifth,shriver2021dnnv}.
  \item \textbf{Empirical insights on \emph{verified} robustness.} We show that scale-invariant properties are substantially more verifiable than strict label consistency, and that adversarial training measurably improves verified safety in this setting (with solver-specific effects on runtime/unknowns), substantiated by the full matrix of results in Table~\ref{tab:all_models_props}.
  \item \textbf{Reproducibility.} We document environment, specifications, and tool configurations to enable reproduction and extension of our study.
\end{itemize}

The remainder of this paper is structured as follows. 
Section~\ref{sec:related_work} reviews related work on neural network verification, verification tools, and verification in autonomous driving contexts. 
Section~\ref{sec:methodology} describes the Unity-based simulation environment, dataset collection process, network architectures, and the robustness properties under consideration. 
Section~\ref{sec:verification} outlines the verification tools and experimental setup. 
Section~\ref{sec:results} presents and analyses the verification results across models, training regimes, and specifications. 
Section~\ref{sec:discussion} discusses the implications of our findings for verification workflows in ADS development. 
Finally, Section~\ref{sec:conclusion} concludes and highlights future research directions.
